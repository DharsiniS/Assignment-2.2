Explain in detail:
*  HDFS

	Hadoop Distributed File System
	This is a java-based file system that was designed for storing large clusters of servers.
	This is also provides reliable and scalable data storage and also easier access.
	This follows a master-slave architecture.
	This stores the meta data on name noda and application data on data node.
	HDFS targets at fault detection and recovery, huge datasets, hardware at data 	

*  Hadoop cluster

	Hadoop cluster is a special type of computational cluster designed for storing and analyzing tremendous amount of unstructured data in a distributed computing environment.
	These clusters run on low cost commodity computers. 
	This has three components client,master,slave.
	This comprises of three different types of nodes name, master nodes, worker nodes and client nodes.
	These are known for boosting the speed of data analysis applications.They are highly scalable.
	These are highly resistant to failure because each piece of data is copied onto other cluster nodes which ensures data is not lost if one node fails.
	These are also known as "shared nothing"  systems because the only thinbg thats shared between them is network that connects them.
	

* HDFS Blocks
	
	HDFS stores data in terms of blocks. The block size is very large and the default size is 64MB.
	The files are split into 64MB and only then they are stored into hadoop filesystem.
	This simplifies the stroage of datanodes.
	These are of fixed size so it is easy to calculate the number of blocks on a single disk.